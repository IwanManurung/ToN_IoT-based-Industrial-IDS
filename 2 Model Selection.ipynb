{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb1fd45",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c80f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: A restricted method in java.lang.System has been called\n",
      "WARNING: java.lang.System::load has been called by org.jpype.JPypeContext in an unnamed module (file:/Users/iwan/Attraction/Venture/p3apps/.venv/lib/python3.12/site-packages/org.jpype.jar)\n",
      "WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\n",
      "WARNING: Restricted methods will be blocked in a future release unless native access is enabled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from capymoa.stream import NumpyStream\n",
    "from src import iids_util\n",
    "from typing import List, Dict, Union, Literal\n",
    "from base import util\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649848e",
   "metadata": {},
   "source": [
    "### Loading sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b5d1c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('precalc/edge_fridge_StandardScaler_60.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e466825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(x) - set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67033a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dtype:  float64\n"
     ]
    }
   ],
   "source": [
    "feature, target, header = iids_util.loading_edge_dataset(\n",
    "    base_device='modbus', \n",
    "    all_in_fusion=True,\n",
    "    load_all=False,\n",
    "    low_memory=False,\n",
    "    sample_size=.1,\n",
    "    random_seed=80)\n",
    "\n",
    "scaled = iids_util.online_normalization(data=feature,\n",
    "                                        window_size=60,\n",
    "                                        scaler_model='StandardScaler')\n",
    "\n",
    "target = iids_util.map_as_binary_class(ntarget=target,\n",
    "                                       class_0='normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9607cf",
   "metadata": {},
   "source": [
    "### Make dataset as streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc23dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stream_cls = NumpyStream(X=scaled, y=target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc2d9e",
   "metadata": {},
   "source": [
    "### Draws simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Classifier Models\n",
    "def run_classifier(stream_cls: NumpyStream,\n",
    "                   model: str,\n",
    "                   random_seed: int =1,\n",
    "                   time_limit: int =120):\n",
    "    \n",
    "    learner = iids_util.load_classifier_model(stream=stream_cls,\n",
    "                                              method=model,\n",
    "                                              random_seed=random_seed)\n",
    "    \n",
    "    stream_cls.restart()\n",
    "    instance_seen = 0 # counter for evaluated instances\n",
    "    err_instances = 0\n",
    "    class_0_count = 0 # counter for predicted class as 0\n",
    "    class_1_count = 0 # counter for predicted class as 1\n",
    "    y_true = [] # Actual label from groundtruth\n",
    "    y_pred = [] # Predicted label by model\n",
    "    start = time.time() # runtime started at the beginning of the test-then-train loops\n",
    "\n",
    "    while stream_cls.has_more_instances():\n",
    "        curr_ins = stream_cls.next_instance()\n",
    "        try:\n",
    "            # test\n",
    "            predict = learner.predict(curr_ins)\n",
    "            # If model predict None, then always predict majority class\n",
    "            if class_1_count > class_0_count:\n",
    "                majority_class = 1\n",
    "            else:\n",
    "                majority_class = 0\n",
    "\n",
    "            if predict == None:\n",
    "                predict = majority_class\n",
    "            \n",
    "            # update majority class counter\n",
    "            if predict == 0:\n",
    "                class_0_count += 1\n",
    "            elif predict == 1:\n",
    "                class_1_count += 1\n",
    "\n",
    "            # evaluator.update(curr_ins.y_index, predict)\n",
    "            y_true.append(curr_ins.y_index)\n",
    "            y_pred.append(predict)\n",
    "            # train\n",
    "            learner.train(curr_ins)\n",
    "            instance_seen += 1\n",
    "        except:\n",
    "            print(\"Error Instance:\", curr_ins, end='\\r', flush=True)\n",
    "            err_instances += 1\n",
    "\n",
    "        if (instance_seen % 100 == 0) or (stream_cls.has_more_instances() == False):\n",
    "            meter = instance_seen / stream_cls._len\n",
    "            msg = util.progress_meter(progress=meter)\n",
    "            print(f\"{msg}. {model}: instance_seen:{instance_seen:,}. Error instances: {err_instances:,}\", end='\\r', flush=True)\n",
    "\n",
    "        # check if run time is too long\n",
    "        runtime = time.time() - start\n",
    "        if runtime > time_limit:\n",
    "            evaluator = iids_util.evaluation_metrics(y_pred=y_pred, y_true=y_true)\n",
    "            evaluator.update({'instance_seen': instance_seen})\n",
    "            evaluator.update({'runtime': round(runtime,3)})\n",
    "            return evaluator\n",
    "\n",
    "    runtime = time.time() - start\n",
    "    evaluator = iids_util.evaluation_metrics(y_pred=y_pred, y_true=y_true)\n",
    "    evaluator.update({'instance_seen': instance_seen})\n",
    "    evaluator.update({'runtime': round(runtime,3)})\n",
    "\n",
    "    return evaluator\n",
    "\n",
    "# Function for Anomaly Detector\n",
    "def run_detector(stream_cls: NumpyStream,\n",
    "                 model: str,\n",
    "                 random_seed: int =1,\n",
    "                 time_limit: int =120):\n",
    "    learner = iids_util.load_anomaly_model(stream=stream_cls,\n",
    "                                           method=model,\n",
    "                                           random_seed=random_seed)\n",
    "    stream_cls.restart()\n",
    "    instance_seen = 0 # counter for evaluated instances\n",
    "    err_instances = 0\n",
    "    y_true = [] # Actual label from groundtruth\n",
    "    y_pred = [] # Predicted label by model\n",
    "    start = time.time() # runtime started at the beginning of the test-then-train loops\n",
    "    while stream_cls.has_more_instances():\n",
    "        try:\n",
    "            curr_ins = stream_cls.next_instance()\n",
    "            # test\n",
    "            score = learner.score_instance(curr_ins)\n",
    "            y_scores = [score, score, score]\n",
    "            y_models = iids_util.proba_prediction_rules(nscore=y_scores)\n",
    "            y_predict = iids_util.voting_decision(npredicts=y_models)\n",
    "            # train\n",
    "            learner.train(curr_ins)\n",
    "            # update results\n",
    "            y_true.append(curr_ins.y_index)\n",
    "            y_pred.append(y_predict)\n",
    "            instance_seen += 1\n",
    "        except:\n",
    "            print(\"Error Instance:\", curr_ins, end='\\r', flush=True)\n",
    "            err_instances += 1\n",
    "        \n",
    "        if (instance_seen % 100 == 0) or (stream_cls.has_more_instances() == False):\n",
    "            meter = instance_seen / stream_cls._len\n",
    "            msg = util.progress_meter(progress=meter)\n",
    "            print(f\"{msg}. {model}: instance_seen:{instance_seen:,}. Error Instances: {err_instances:,}\", end='\\r', flush=True)\n",
    "\n",
    "        # check if run time is too long\n",
    "        runtime = time.time() - start\n",
    "        if runtime > time_limit:\n",
    "            evaluator = iids_util.evaluation_metrics(y_pred=y_pred, y_true=y_true)\n",
    "            evaluator.update({'instance_seen': instance_seen})\n",
    "            evaluator.update({'runtime': round(runtime,3)})\n",
    "            return evaluator\n",
    "        \n",
    "    runtime = time.time() - start\n",
    "    evaluator = iids_util.evaluation_metrics(y_pred=y_pred, y_true=y_true)\n",
    "    evaluator.update({'instance_seen': instance_seen})\n",
    "    evaluator.update({'runtime': round(runtime,3)})\n",
    "    \n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6395b",
   "metadata": {},
   "source": [
    "### Run simulations - Classifier Models\n",
    "Available models were inherit from CapyMOA.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a262704",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_models = [\"AdaptiveRandomForestClassifier\", \"DynamicWeightedMajority\", \"EFDT\",\n",
    "          \"HoeffdingAdaptiveTree\", \"KNN\", \"LeveragingBagging\", \"NaiveBayes\",\n",
    "          \"OnlineAdwinBagging\", \"OnlineBagging\", \"OnlineSmoothBoost\",\n",
    "          \"OzaBoost\", \"PassiveAggressiveClassifier\", \"SGDClassifier\",\n",
    "          \"StreamingGradientBoostedTrees\", \"StreamingRandomPatches\",\n",
    "          \"HoeffdingTree\"]\n",
    "\n",
    "classifier_evaluator = {}\n",
    "\n",
    "for model in classifier_models:\n",
    "    evaluator = run_classifier(stream_cls=stream_cls,\n",
    "                               model=model,\n",
    "                               random_seed=80,\n",
    "                               time_limit=120)\n",
    "    \n",
    "    classifier_evaluator.update({model: evaluator})\n",
    "    del evaluator\n",
    "\n",
    "eval_fname = f'output/Classifier_Evaluation_Table.json'\n",
    "with open(eval_fname, 'w') as file:\n",
    "    json.dump(classifier_evaluator, file)\n",
    "\n",
    "c_table = pd.DataFrame()\n",
    "for model in classifier_evaluator.keys():\n",
    "    for metrics in classifier_evaluator.get(model).keys():\n",
    "        c_table.loc[model, metrics] = classifier_evaluator.get(model).get(metrics)\n",
    "\n",
    "c_table = c_table.sort_values('MCC', ascending=False).reset_index(names='Classifier')\n",
    "c_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a372c45",
   "metadata": {},
   "source": [
    "### Run simulations - Anomaly Models\n",
    "Available models were inherit from CapyMOA.anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b2fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_models = [\"Autoencoder\",\"HalfSpaceTrees\", \"OnlineIsolationForest\",\n",
    "          \"StreamRHF\", \"StreamingIsolationForest\", \"RobustRandomCutForest\",\n",
    "          \"AdaptiveIsolationForest\"]\n",
    "\n",
    "detector_evaluator = {}\n",
    "stream_cls.restart()\n",
    "\n",
    "for model in anomaly_models:\n",
    "    evaluator = run_detector(stream_cls=stream_cls,\n",
    "                             model=model,\n",
    "                             random_seed=80,\n",
    "                             time_limit=120)\n",
    "    \n",
    "    detector_evaluator.update({model: evaluator})\n",
    "    del evaluator\n",
    "\n",
    "eval_fname = f'output/Anomaly_Evaluation_Table.json'\n",
    "with open(eval_fname, 'w') as file:\n",
    "    json.dump(detector_evaluator, file)\n",
    "\n",
    "a_table = pd.DataFrame()\n",
    "for model in detector_evaluator.keys():\n",
    "    for metrics in detector_evaluator.get(model).keys():\n",
    "        a_table.loc[model, metrics] = detector_evaluator.get(model).get(metrics)\n",
    "\n",
    "a_table = a_table.sort_values('MCC', ascending=False).reset_index(names='Detector')\n",
    "a_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
